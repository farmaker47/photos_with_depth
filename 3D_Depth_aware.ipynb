{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3D_Depth_aware.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytTOMbYUQgGh"
      },
      "source": [
        "# Depth estimation of a Pytorch model \n",
        "\n",
        "The below procedure helps converting a Pytorch model which is taken from original paper [here](https://syncedreview.com/2020/04/13/ai-transforms-rgb-d-images-into-an-impressive-3d-format/) to TensorFlow Lite so the model can be used inside an android application. The Pytorch model's code is taken from [this](https://github.com/vt-vl-lab/3d-photo-inpainting) github repository. This model takes as input an RGB image of shape [1, 3, 384, 384] and outputs a float array of shape [1, 1, 384, 384] which is converted to a grayscale image. Inference with the TensorFlow Lite model takes place with the same procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WCoQdjgSsaz"
      },
      "source": [
        "### First load necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmVTxdPgQScq",
        "outputId": "9808bbb5-ce68-4e81-b3f7-1d5fbbdf4417"
      },
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "!pip install pip install git+https://github.com/onnx/onnx-tensorflow.git\n",
        "\n",
        "\n",
        "import gdown\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from torchvision import models\n",
        "from collections import OrderedDict\n",
        "import onnx\n",
        "import onnxruntime\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/38/57/65f48111f823df02da3e391b0b1aaadaf9972f8aa68ab3a41f46d59f57fe/onnx-1.8.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 268kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->onnx) (54.2.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.8.1\n",
            "Collecting onnxruntime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/f0/666d6e3ceaa276a54e728f9972732e058544cbb6a3e1a778a8d6f87132c1/onnxruntime-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (54.2.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.7.0\n",
            "Collecting git+https://github.com/onnx/onnx-tensorflow.git\n",
            "  Cloning https://github.com/onnx/onnx-tensorflow.git to /tmp/pip-req-build-77100chb\n",
            "  Running command git clone -q https://github.com/onnx/onnx-tensorflow.git /tmp/pip-req-build-77100chb\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (19.3.1)\n",
            "Collecting install\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/a5/fd2eb807a9a593869ee8b7a6bcb4ad84a6eb31cef5c24d1bfbf7c938c13f/install-1.3.4-py3-none-any.whl\n",
            "Requirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from onnx-tf==1.7.0) (1.8.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from onnx-tf==1.7.0) (3.13)\n",
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/e3/56d2fe76f0bb7c88ed9b2a6a557e25e83e252aec08f13de34369cd850a0b/tensorflow_addons-0.12.1-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->onnx-tf==1.7.0) (3.12.4)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->onnx-tf==1.7.0) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->onnx-tf==1.7.0) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx>=1.7.0->onnx-tf==1.7.0) (1.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow_addons->onnx-tf==1.7.0) (2.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->onnx>=1.7.0->onnx-tf==1.7.0) (54.2.0)\n",
            "Building wheels for collected packages: onnx-tf\n",
            "  Building wheel for onnx-tf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnx-tf: filename=onnx_tf-1.7.0-cp37-none-any.whl size=219452 sha256=79f487205a95c4a2ba7f91d2b0dcc2dc3873350a37c9bff3ee7790d0f99eb4d9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sc9g9djc/wheels/54/24/31/8873b33d2d560efdfa1ed6f346df67ef793b1897358705a480\n",
            "Successfully built onnx-tf\n",
            "Installing collected packages: install, tensorflow-addons, onnx-tf\n",
            "Successfully installed install-1.3.4 onnx-tf-1.7.0 tensorflow-addons-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0b7K92ZS6zd"
      },
      "source": [
        "### Helper function to load the model's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsffa_znSXB0"
      },
      "source": [
        "def copyStateDict(state_dict):\n",
        "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
        "        start_idx = 1\n",
        "    else:\n",
        "        start_idx = 0\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        name = \".\".join(k.split(\".\")[start_idx:])\n",
        "        new_state_dict[name] = v\n",
        "    return new_state_dict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dga8qiG8TAFl"
      },
      "source": [
        "### Download model's weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWrU-PpvSlSi",
        "outputId": "2a31f27f-35df-492d-f0c1-27d17379e3f6"
      },
      "source": [
        "!wget https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/model.pt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-01 16:15:14--  https://filebox.ece.vt.edu/~jbhuang/project/3DPhoto/model/model.pt\n",
            "Resolving filebox.ece.vt.edu (filebox.ece.vt.edu)... 128.173.88.43\n",
            "Connecting to filebox.ece.vt.edu (filebox.ece.vt.edu)|128.173.88.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149751722 (143M)\n",
            "Saving to: ‘model.pt’\n",
            "\n",
            "model.pt            100%[===================>] 142.81M  14.5MB/s    in 19s     \n",
            "\n",
            "2021-04-01 16:15:34 (7.44 MB/s) - ‘model.pt’ saved [149751722/149751722]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_rHlnChTFS7"
      },
      "source": [
        "### Model's architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOA17Z1qS5tM"
      },
      "source": [
        "class MonoDepthNet(nn.Module):\n",
        "    \"\"\"Network for monocular depth estimation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, path=None, features=256):\n",
        "        \"\"\"Init.\n",
        "        Args:\n",
        "            path (str, optional): Path to saved model. Defaults to None.\n",
        "            features (int, optional): Number of features. Defaults to 256.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        resnet = models.resnet50(pretrained=False)\n",
        "\n",
        "        self.pretrained = nn.Module()\n",
        "        self.scratch = nn.Module()\n",
        "        self.pretrained.layer1 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu,\n",
        "                                               resnet.maxpool, resnet.layer1)\n",
        "\n",
        "        self.pretrained.layer2 = resnet.layer2\n",
        "        self.pretrained.layer3 = resnet.layer3\n",
        "        self.pretrained.layer4 = resnet.layer4\n",
        "\n",
        "        # adjust channel number of feature maps\n",
        "        self.scratch.layer1_rn = nn.Conv2d(256, features, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.scratch.layer2_rn = nn.Conv2d(512, features, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.scratch.layer3_rn = nn.Conv2d(1024, features, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.scratch.layer4_rn = nn.Conv2d(2048, features, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.scratch.refinenet4 = FeatureFusionBlock(features)\n",
        "        self.scratch.refinenet3 = FeatureFusionBlock(features)\n",
        "        self.scratch.refinenet2 = FeatureFusionBlock(features)\n",
        "        self.scratch.refinenet1 = FeatureFusionBlock(features)\n",
        "\n",
        "        # adaptive output module: 2 convolutions and upsampling\n",
        "        self.scratch.output_conv = nn.Sequential(nn.Conv2d(features, 128, kernel_size=3, stride=1, padding=1),\n",
        "                                                 nn.Conv2d(128, 1, kernel_size=3, stride=1, padding=1),\n",
        "                                                 Interpolate(scale_factor=2, mode='bilinear'))\n",
        "\n",
        "        # load model\n",
        "        if path:\n",
        "            self.load(path)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        Args:\n",
        "            x (tensor): input data (image)\n",
        "        Returns:\n",
        "            tensor: depth\n",
        "        \"\"\"\n",
        "        layer_1 = self.pretrained.layer1(x)\n",
        "        layer_2 = self.pretrained.layer2(layer_1)\n",
        "        layer_3 = self.pretrained.layer3(layer_2)\n",
        "        layer_4 = self.pretrained.layer4(layer_3)\n",
        "\n",
        "        layer_1_rn = self.scratch.layer1_rn(layer_1)\n",
        "        layer_2_rn = self.scratch.layer2_rn(layer_2)\n",
        "        layer_3_rn = self.scratch.layer3_rn(layer_3)\n",
        "        layer_4_rn = self.scratch.layer4_rn(layer_4)\n",
        "\n",
        "        path_4 = self.scratch.refinenet4(layer_4_rn)\n",
        "        path_3 = self.scratch.refinenet3(path_4, layer_3_rn)\n",
        "        path_2 = self.scratch.refinenet2(path_3, layer_2_rn)\n",
        "        path_1 = self.scratch.refinenet1(path_2, layer_1_rn)\n",
        "\n",
        "        out = self.scratch.output_conv(path_1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def load(self, path):\n",
        "        \"\"\"Load model from file.\n",
        "        Args:\n",
        "            path (str): file path\n",
        "        \"\"\"\n",
        "        parameters = torch.load(path)\n",
        "\n",
        "        self.load_state_dict(parameters)\n",
        "\n",
        "\n",
        "class Interpolate(nn.Module):\n",
        "    \"\"\"Interpolation module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale_factor, mode):\n",
        "        \"\"\"Init.\n",
        "        Args:\n",
        "            scale_factor (float): scaling\n",
        "            mode (str): interpolation mode\n",
        "        \"\"\"\n",
        "        super(Interpolate, self).__init__()\n",
        "\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.scale_factor = scale_factor\n",
        "        self.mode = mode\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        Args:\n",
        "            x (tensor): input\n",
        "        Returns:\n",
        "            tensor: interpolated data\n",
        "        \"\"\"\n",
        "        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=False)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualConvUnit(nn.Module):\n",
        "    \"\"\"Residual convolution module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features):\n",
        "        \"\"\"Init.\n",
        "        Args:\n",
        "            features (int): number of features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        Args:\n",
        "            x (tensor): input\n",
        "        Returns:\n",
        "            tensor: output\n",
        "        \"\"\"\n",
        "        out = self.relu(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "\n",
        "        return out + x\n",
        "\n",
        "\n",
        "class FeatureFusionBlock(nn.Module):\n",
        "    \"\"\"Feature fusion block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, features):\n",
        "        \"\"\"Init.\n",
        "        Args:\n",
        "            features (int): number of features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.resConfUnit = ResidualConvUnit(features)\n",
        "\n",
        "    def forward(self, *xs):\n",
        "        \"\"\"Forward pass.\n",
        "        Returns:\n",
        "            tensor: output\n",
        "        \"\"\"\n",
        "        output = xs[0]\n",
        "\n",
        "        if len(xs) == 2:\n",
        "            output += self.resConfUnit(xs[1])\n",
        "\n",
        "        output = self.resConfUnit(output)\n",
        "        output = nn.functional.interpolate(output, scale_factor=2,\n",
        "                                           mode='bilinear')\n",
        "\n",
        "        return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psUy9z5bTZKS"
      },
      "source": [
        "### Load model's weights and print the architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0IEd2c8Tilt",
        "outputId": "52e7c21d-cd24-4d76-e015-a8c0ce88ec7c"
      },
      "source": [
        "\n",
        "net = MonoDepthNet()\n",
        "net.load_state_dict(copyStateDict(torch.load('/content/model.pt', map_location='cpu')))\n",
        "\n",
        "print(\"Model loaded\")\n",
        "net.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MonoDepthNet(\n",
              "  (pretrained): Module(\n",
              "    (layer1): Sequential(\n",
              "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (scratch): Module(\n",
              "    (layer1_rn): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer2_rn): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer3_rn): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer4_rn): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (refinenet4): FeatureFusionBlock(\n",
              "      (resConfUnit): ResidualConvUnit(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (refinenet3): FeatureFusionBlock(\n",
              "      (resConfUnit): ResidualConvUnit(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (refinenet2): FeatureFusionBlock(\n",
              "      (resConfUnit): ResidualConvUnit(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (refinenet1): FeatureFusionBlock(\n",
              "      (resConfUnit): ResidualConvUnit(\n",
              "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (output_conv): Sequential(\n",
              "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (1): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (2): Interpolate()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2VunT5mTgEJ"
      },
      "source": [
        "### Pre and post processing of the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWreOgbMUY2U"
      },
      "source": [
        "\n",
        "def read_image(path):\n",
        "    \"\"\"Read image and output RGB image (0-1).\n",
        "    Args:\n",
        "        path (str): path to file\n",
        "    Returns:\n",
        "        array: RGB image (0-1)\n",
        "    \"\"\"\n",
        "    img = cv2.imread(path)\n",
        "\n",
        "    if img.ndim == 2:\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "def resize_image(img):\n",
        "    \"\"\"Resize image and make it fit for network.\n",
        "    Args:\n",
        "        img (array): image\n",
        "    Returns:\n",
        "        tensor: data ready for network\n",
        "    \"\"\"\n",
        "    height_orig = img.shape[0]\n",
        "    width_orig = img.shape[1]\n",
        "    unit_scale = 384.\n",
        "\n",
        "    if width_orig > height_orig:\n",
        "        scale = width_orig / unit_scale\n",
        "    else:\n",
        "        scale = height_orig / unit_scale\n",
        "\n",
        "    height = (np.ceil(height_orig / scale / 32) * 32).astype(int)\n",
        "    width = (np.ceil(width_orig / scale / 32) * 32).astype(int)\n",
        "\n",
        "    img_resized = cv2.resize(img, (width, height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    img_resized = (\n",
        "        torch.from_numpy(np.transpose(img_resized, (2, 0, 1))).contiguous().float()\n",
        "    )\n",
        "    img_resized = img_resized.unsqueeze(0)\n",
        "\n",
        "    return img_resized\n",
        "\n",
        "\n",
        "def resize_depth(depth, width, height):\n",
        "    \"\"\"Resize depth map and bring to CPU (numpy).\n",
        "    Args:\n",
        "        depth (tensor): depth\n",
        "        width (int): image width\n",
        "        height (int): image height\n",
        "    Returns:\n",
        "        array: processed depth\n",
        "    \"\"\"\n",
        "    print(depth)\n",
        "    depth = torch.squeeze(depth[0, :, :, :]).to(\"cpu\")\n",
        "    print(depth.shape)\n",
        "    depth = cv2.blur(depth.numpy(), (3, 3))\n",
        "    print(depth)\n",
        "    depth_resized = cv2.resize(\n",
        "        depth, (width, height), interpolation=cv2.INTER_LINEAR\n",
        "    )\n",
        "\n",
        "    return depth_resized\n",
        "\n",
        "def write_depth(path, depth, bits=1):\n",
        "    \"\"\"Write depth map to pfm and png file.\n",
        "    Args:\n",
        "        path (str): filepath without extension\n",
        "        depth (array): depth\n",
        "    \"\"\"\n",
        "    # write_pfm(path + \".pfm\", depth.astype(np.float32))\n",
        "\n",
        "    depth_min = depth.min()\n",
        "    depth_max = depth.max()\n",
        "    print(depth_min)\n",
        "    print(depth_max)\n",
        "\n",
        "    max_val = (2**(8*bits))-1\n",
        "    print(max_val)\n",
        "\n",
        "    if depth_max - depth_min > np.finfo(\"float\").eps:\n",
        "        out = max_val * (depth - depth_min) / (depth_max - depth_min)\n",
        "\n",
        "        # or use 255 * depth as below line\n",
        "        #out = max_val * depth\n",
        "    else:\n",
        "        out = 0\n",
        "\n",
        "    print(out)\n",
        "\n",
        "    if bits == 1:\n",
        "        cv2.imwrite(path + \".png\", out.astype(\"uint8\"))\n",
        "    elif bits == 2:\n",
        "        cv2.imwrite(path + \".png\", out.astype(\"uint16\"))\n",
        "        \n",
        "    return"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpQYAQR1TvGS"
      },
      "source": [
        "### Load moon.png to use it for inference (this image can be found at images folder of the github's master branch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCyFadiGUQUf",
        "outputId": "51e23ea2-6f0c-4cac-facc-12e0b9cb8592"
      },
      "source": [
        "batch_size = 1\n",
        "# Input to the model\n",
        "# x = torch.randn(batch_size, 3, 384, 384, requires_grad=True)\n",
        "# onnx_runtime_input = x.detach().numpy()\n",
        "\n",
        "x = read_image(\"/content/moon.jpg\")\n",
        "x = resize_image(x)\n",
        "onnx_runtime_input = x\n",
        "\n",
        "t1 = datetime.now()\n",
        "print(x.shape)\n",
        "print(x)\n",
        "print(x[0][0][0])\n",
        "torch_out = net(x)\n",
        "print(\"Torch Out shape: {}\".format(torch_out.detach().numpy().shape))\n",
        "t2 = datetime.now()\n",
        "print(\"Time taken for Pytoch model\", str(t2-t1))\n",
        "store_out = torch_out[0].detach().numpy()\n",
        "print(torch_out.detach().numpy())\n",
        "print(\"Model ran succesfully\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 384, 384])\n",
            "tensor([[[[0.0108, 0.0059, 0.0210,  ..., 0.0047, 0.0051, 0.0196],\n",
            "          [0.0266, 0.0265, 0.0184,  ..., 0.0242, 0.0337, 0.0305],\n",
            "          [0.0025, 0.0064, 0.0145,  ..., 0.0093, 0.0216, 0.0136],\n",
            "          ...,\n",
            "          [0.0222, 0.0193, 0.0050,  ..., 0.0011, 0.0360, 0.0257],\n",
            "          [0.0425, 0.0219, 0.0356,  ..., 0.0489, 0.0025, 0.0018],\n",
            "          [0.0142, 0.0073, 0.0413,  ..., 0.0246, 0.0053, 0.0399]],\n",
            "\n",
            "         [[0.0108, 0.0059, 0.0210,  ..., 0.0047, 0.0051, 0.0157],\n",
            "          [0.0266, 0.0265, 0.0184,  ..., 0.0242, 0.0337, 0.0296],\n",
            "          [0.0025, 0.0064, 0.0145,  ..., 0.0093, 0.0216, 0.0166],\n",
            "          ...,\n",
            "          [0.0222, 0.0193, 0.0050,  ..., 0.0011, 0.0360, 0.0266],\n",
            "          [0.0425, 0.0219, 0.0356,  ..., 0.0489, 0.0025, 0.0090],\n",
            "          [0.0142, 0.0073, 0.0419,  ..., 0.0324, 0.0132, 0.0030]],\n",
            "\n",
            "         [[0.0108, 0.0059, 0.0210,  ..., 0.0047, 0.0051, 0.0452],\n",
            "          [0.0266, 0.0265, 0.0184,  ..., 0.0242, 0.0337, 0.0401],\n",
            "          [0.0025, 0.0064, 0.0145,  ..., 0.0093, 0.0216, 0.0008],\n",
            "          ...,\n",
            "          [0.0144, 0.0115, 0.0011,  ..., 0.0025, 0.0373, 0.0552],\n",
            "          [0.0347, 0.0141, 0.0277,  ..., 0.0411, 0.0000, 0.0000],\n",
            "          [0.0217, 0.0073, 0.0322,  ..., 0.0193, 0.0086, 0.0205]]]])\n",
            "tensor([0.0108, 0.0059, 0.0210, 0.0098, 0.0013, 0.0213, 0.0248, 0.0127, 0.0100,\n",
            "        0.0327, 0.0152, 0.0102, 0.0008, 0.0042, 0.0100, 0.0044, 0.0000, 0.0085,\n",
            "        0.0010, 0.0040, 0.0130, 0.0046, 0.0000, 0.0145, 0.0134, 0.0338, 0.0051,\n",
            "        0.0121, 0.0257, 0.0153, 0.0073, 0.0175, 0.0076, 0.0157, 0.0121, 0.0053,\n",
            "        0.0118, 0.0086, 0.0031, 0.0048, 0.0137, 0.0176, 0.0192, 0.0404, 0.0198,\n",
            "        0.0045, 0.0219, 0.0433, 0.0157, 0.0069, 0.0319, 0.0234, 0.0205, 0.0158,\n",
            "        0.0095, 0.0474, 0.0195, 0.0251, 0.0068, 0.0024, 0.0100, 0.0175, 0.0226,\n",
            "        0.0219, 0.0110, 0.0122, 0.0175, 0.0386, 0.0238, 0.0065, 0.0045, 0.0100,\n",
            "        0.0183, 0.0183, 0.0182, 0.0170, 0.0189, 0.0254, 0.0156, 0.0182, 0.0159,\n",
            "        0.0284, 0.0265, 0.0111, 0.0114, 0.0070, 0.0165, 0.0229, 0.0352, 0.0133,\n",
            "        0.0140, 0.0256, 0.0336, 0.0100, 0.0335, 0.0309, 0.0054, 0.0301, 0.0201,\n",
            "        0.0525, 0.0085, 0.0224, 0.0221, 0.0192, 0.0205, 0.0285, 0.0030, 0.0137,\n",
            "        0.0599, 0.0317, 0.0276, 0.0183, 0.0039, 0.0090, 0.0214, 0.0251, 0.0209,\n",
            "        0.0203, 0.0188, 0.0148, 0.0167, 0.0111, 0.0193, 0.0098, 0.0084, 0.0237,\n",
            "        0.0163, 0.0233, 0.0237, 0.0480, 0.0494, 0.0209, 0.0189, 0.0308, 0.0277,\n",
            "        0.0199, 0.0432, 0.0478, 0.0505, 0.0232, 0.0395, 0.0190, 0.0136, 0.0219,\n",
            "        0.0331, 0.0220, 0.0212, 0.0385, 0.0229, 0.0517, 0.0227, 0.0508, 0.0410,\n",
            "        0.0654, 0.0231, 0.0467, 0.0359, 0.0511, 0.0351, 0.0238, 0.0249, 0.0240,\n",
            "        0.0755, 0.0596, 0.0302, 0.0705, 0.0619, 0.0389, 0.0551, 0.0214, 0.0155,\n",
            "        0.0563, 0.0421, 0.0329, 0.0478, 0.0233, 0.0218, 0.0104, 0.0310, 0.0257,\n",
            "        0.0153, 0.0590, 0.0352, 0.0427, 0.0594, 0.0197, 0.0338, 0.0217, 0.0426,\n",
            "        0.0287, 0.0390, 0.0309, 0.0366, 0.0201, 0.0170, 0.0313, 0.0446, 0.0494,\n",
            "        0.0470, 0.0560, 0.0593, 0.0317, 0.0654, 0.0739, 0.0639, 0.0755, 0.0842,\n",
            "        0.0574, 0.0843, 0.0635, 0.0749, 0.0918, 0.0890, 0.0736, 0.0871, 0.0863,\n",
            "        0.0805, 0.0968, 0.0719, 0.0864, 0.0784, 0.0859, 0.0975, 0.0768, 0.1087,\n",
            "        0.0431, 0.0757, 0.0619, 0.0714, 0.0366, 0.0812, 0.0438, 0.0460, 0.0660,\n",
            "        0.0384, 0.0504, 0.0236, 0.0418, 0.0506, 0.0482, 0.0531, 0.0604, 0.0300,\n",
            "        0.0371, 0.0428, 0.0321, 0.0131, 0.0280, 0.0633, 0.0481, 0.0588, 0.0894,\n",
            "        0.0501, 0.0409, 0.0183, 0.0252, 0.0426, 0.0398, 0.0261, 0.0561, 0.0281,\n",
            "        0.0374, 0.0137, 0.0244, 0.0264, 0.0368, 0.0361, 0.0454, 0.0462, 0.0088,\n",
            "        0.0256, 0.0354, 0.0235, 0.0203, 0.0331, 0.0594, 0.0285, 0.0168, 0.0228,\n",
            "        0.0319, 0.0411, 0.0028, 0.0271, 0.0417, 0.0349, 0.0558, 0.0202, 0.0353,\n",
            "        0.0131, 0.0480, 0.0247, 0.0128, 0.0155, 0.0157, 0.0132, 0.0048, 0.0500,\n",
            "        0.0358, 0.0196, 0.0286, 0.0383, 0.0124, 0.0245, 0.0128, 0.0207, 0.0114,\n",
            "        0.0020, 0.0189, 0.0311, 0.0495, 0.0367, 0.0182, 0.0250, 0.0105, 0.0309,\n",
            "        0.0323, 0.0322, 0.0347, 0.0294, 0.0410, 0.0279, 0.0157, 0.0060, 0.0160,\n",
            "        0.0153, 0.0256, 0.0348, 0.0110, 0.0283, 0.0220, 0.0243, 0.0299, 0.0456,\n",
            "        0.0408, 0.0126, 0.0481, 0.0208, 0.0020, 0.0415, 0.0330, 0.0000, 0.0290,\n",
            "        0.0359, 0.0118, 0.0138, 0.0153, 0.0174, 0.0182, 0.0253, 0.0236, 0.0090,\n",
            "        0.0062, 0.0041, 0.0093, 0.0058, 0.0530, 0.0120, 0.0115, 0.0198, 0.0188,\n",
            "        0.0230, 0.0203, 0.0183, 0.0284, 0.0233, 0.0031, 0.0140, 0.0093, 0.0055,\n",
            "        0.0036, 0.0232, 0.0086, 0.0133, 0.0039, 0.0239, 0.0176, 0.0187, 0.0266,\n",
            "        0.0309, 0.0176, 0.0236, 0.0047, 0.0051, 0.0196])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Torch Out shape: (1, 1, 384, 384)\n",
            "Time taken for Pytoch model 0:00:02.526465\n",
            "[[[[-0.39669645 -0.3981164  -0.4009563  ... -0.38015398 -0.36844572\n",
            "    -0.3625916 ]\n",
            "   [-0.40167084 -0.4031891  -0.4062256  ... -0.38171238 -0.37091136\n",
            "    -0.36551085]\n",
            "   [-0.41161963 -0.41333452 -0.41676426 ... -0.3848292  -0.3758427\n",
            "    -0.37134945]\n",
            "   ...\n",
            "   [ 1.1138916   1.1200587   1.1323925  ...  1.0947251   1.0689293\n",
            "     1.0560315 ]\n",
            "   [ 1.0912312   1.0984172   1.112789   ...  1.086489    1.0595579\n",
            "     1.0460924 ]\n",
            "   [ 1.079901    1.0875964   1.1029874  ...  1.0823709   1.0548722\n",
            "     1.0411228 ]]]]\n",
            "Model ran succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAHx5vF3T7sb"
      },
      "source": [
        "### Post process the output and write the grayscale image and the numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXh95fXpUsOV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1439482f-6421-4a1b-b3d6-9b22b4ce4026"
      },
      "source": [
        "depth = resize_depth(torch_out.detach(), 480, 640)\n",
        "\n",
        "filename = os.path.join(\n",
        "    \"/content/\", os.path.splitext(os.path.basename(\"chris_grayscale\"))[0]\n",
        ")\n",
        "np.save(filename + '.npy', depth)\n",
        "write_depth(filename, depth, bits=1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.3967, -0.3981, -0.4010,  ..., -0.3802, -0.3684, -0.3626],\n",
            "          [-0.4017, -0.4032, -0.4062,  ..., -0.3817, -0.3709, -0.3655],\n",
            "          [-0.4116, -0.4133, -0.4168,  ..., -0.3848, -0.3758, -0.3713],\n",
            "          ...,\n",
            "          [ 1.1139,  1.1201,  1.1324,  ...,  1.0947,  1.0689,  1.0560],\n",
            "          [ 1.0912,  1.0984,  1.1128,  ...,  1.0865,  1.0596,  1.0461],\n",
            "          [ 1.0799,  1.0876,  1.1030,  ...,  1.0824,  1.0549,  1.0411]]]])\n",
            "torch.Size([384, 384])\n",
            "[[-0.40100303 -0.40199336 -0.40409526 ... -0.37878636 -0.37194005\n",
            "  -0.3682389 ]\n",
            " [-0.404363   -0.405397   -0.40746284 ... -0.37992346 -0.37348303\n",
            "  -0.3699835 ]\n",
            " [-0.41017196 -0.41135567 -0.41353124 ... -0.38226923 -0.37648934\n",
            "  -0.3733405 ]\n",
            " ...\n",
            " [ 1.1152345   1.1194822   1.1287322  ...  1.092637    1.0736377\n",
            "   1.0646838 ]\n",
            " [ 1.0996853   1.1043627   1.1144683  ...  1.0839161   1.0655768\n",
            "   1.0566628 ]\n",
            " [ 1.0923584   1.0972621   1.1078731  ...  1.0807909   1.0625161\n",
            "   1.053476  ]]\n",
            "-0.44207346\n",
            "1.4900764\n",
            "255\n",
            "[[  5.4203663   5.328875    5.1509595 ...   8.804243    9.402558\n",
            "    9.74449  ]\n",
            " [  5.2429895   5.1498823   4.972234  ...   8.7335005   9.317907\n",
            "    9.652389 ]\n",
            " [  4.976926    4.8813996   4.7041407 ...   8.627385    9.190934\n",
            "    9.514242 ]\n",
            " ...\n",
            " [203.47722   203.90933   204.7614    ... 200.18585   198.62274\n",
            "  197.79921  ]\n",
            " [202.89702   203.34169   204.21913   ... 199.94093   198.3774\n",
            "  197.54686  ]\n",
            " [202.51024   202.96327   203.85762   ... 199.77765   198.2138\n",
            "  197.37863  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PElCBNKUDP0"
      },
      "source": [
        "### Export the model to Onnx format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLf4K19PUrVd",
        "outputId": "d94eb736-8a02-4d7c-bd09-67eafb140179"
      },
      "source": [
        "# Export the model\n",
        "torch.onnx.export(net,               # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"3D_depth.onnx\",   # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'] # the model's output names\n",
        "                  )\n",
        "print(\"Model converted succesfully\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3458: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:347: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
            "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
            "We recommend using opset 11 and above for models using this operator. \n",
            "  \"\" + str(_export_onnx_opset_version) + \". \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model converted succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc30ZkBWXmTE",
        "outputId": "3086dd27-0716-40d1-ac02-aa12f87514e5"
      },
      "source": [
        "onnx_model = onnx.load(\"3D_depth.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n",
        "print(\"Model checked succesfully\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model checked succesfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCvQ_e7SXsFs"
      },
      "source": [
        "ort_session = onnxruntime.InferenceSession('3D_depth.onnx')\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    print(tensor)\n",
        "    return tensor.detach().cpu().numpy()\n",
        "\n",
        "ort_inputs = {ort_session.get_inputs()[0].name:onnx_runtime_input}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUPr06JgUKwe"
      },
      "source": [
        "### Convert code to TensrFlow 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05dMviB2Z4UC",
        "outputId": "737154c5-0120-4a7d-aa90-6dcd750a8fbb"
      },
      "source": [
        "onnx_model = onnx.load('3D_depth.onnx')\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph('3D_depth')\n",
        "\n",
        "print(\"Model converted to tensorflow graph succesfully.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: 3D_depth/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: 3D_depth/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model converted to tensorflow graph succesfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFfXWhDkUQ6t"
      },
      "source": [
        "### Convert TensorFlow model to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zy92zXIalwz",
        "outputId": "ff1e945d-1126-48c5-a723-a6737e579518"
      },
      "source": [
        "\n",
        "loaded = tf.saved_model.load('3D_depth')\n",
        "\n",
        "concrete_func = loaded.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]\n",
        "\n",
        "concrete_func.inputs[0].set_shape([None, 3, 384, 384])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])\n",
        "\n",
        "converter.experimental_new_converter = False #error otherwise\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Uncomment this line for float16 quantization.\n",
        "#converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "# Uncomment For Integer Quantization\n",
        "# def representative_data_gen():\n",
        "#     for file in os.listdir(dataset_path)[:10]:\n",
        "#         file_path = dataset_path+file\n",
        "#         image = imgproc.loadImage(file_path)\n",
        "#         image = cv2.resize(image, dsize=(800, 1280), interpolation=cv2.INTER_LINEAR)\n",
        "#         img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, 1280, interpolation=cv2.INTER_LINEAR, mag_ratio=1.5)\n",
        "#         ratio_h = ratio_w = 1 / target_ratio\n",
        "\n",
        "#         # preprocessing\n",
        "#         x = imgproc.normalizeMeanVariance(img_resized)\n",
        "#         x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
        "#         x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
        "#         x = x.cpu().detach().numpy()\n",
        "#         yield [x]\n",
        "\n",
        "# converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tf_lite_model = converter.convert()\n",
        "\n",
        "open('3D_depth.tflite', 'wb').write(tf_lite_model)\n",
        "\n",
        "print(\"Converted to tensorflow lite succesfully.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Please consider switching to the new converter by setting experimental_new_converter=True. The old converter (TOCO) is deprecated.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Converted to tensorflow lite succesfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rg-8RnFpi2Tg"
      },
      "source": [
        "## Inference with TF Lite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_AaLFoyi7lE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40645376-57b1-496a-b219-3c4a9f507d1f"
      },
      "source": [
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"3D_depth.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "#input_shape = input_details[0]['shape']\n",
        "#input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "image_input = read_image(\"/content/moon.jpg\")\n",
        "image_input = resize_image(image_input)\n",
        "print(image_input.shape)\n",
        "print(image_input[0][0][0])\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], image_input)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 384, 384])\n",
            "tensor([0.0108, 0.0059, 0.0210, 0.0098, 0.0013, 0.0213, 0.0248, 0.0127, 0.0100,\n",
            "        0.0327, 0.0152, 0.0102, 0.0008, 0.0042, 0.0100, 0.0044, 0.0000, 0.0085,\n",
            "        0.0010, 0.0040, 0.0130, 0.0046, 0.0000, 0.0145, 0.0134, 0.0338, 0.0051,\n",
            "        0.0121, 0.0257, 0.0153, 0.0073, 0.0175, 0.0076, 0.0157, 0.0121, 0.0053,\n",
            "        0.0118, 0.0086, 0.0031, 0.0048, 0.0137, 0.0176, 0.0192, 0.0404, 0.0198,\n",
            "        0.0045, 0.0219, 0.0433, 0.0157, 0.0069, 0.0319, 0.0234, 0.0205, 0.0158,\n",
            "        0.0095, 0.0474, 0.0195, 0.0251, 0.0068, 0.0024, 0.0100, 0.0175, 0.0226,\n",
            "        0.0219, 0.0110, 0.0122, 0.0175, 0.0386, 0.0238, 0.0065, 0.0045, 0.0100,\n",
            "        0.0183, 0.0183, 0.0182, 0.0170, 0.0189, 0.0254, 0.0156, 0.0182, 0.0159,\n",
            "        0.0284, 0.0265, 0.0111, 0.0114, 0.0070, 0.0165, 0.0229, 0.0352, 0.0133,\n",
            "        0.0140, 0.0256, 0.0336, 0.0100, 0.0335, 0.0309, 0.0054, 0.0301, 0.0201,\n",
            "        0.0525, 0.0085, 0.0224, 0.0221, 0.0192, 0.0205, 0.0285, 0.0030, 0.0137,\n",
            "        0.0599, 0.0317, 0.0276, 0.0183, 0.0039, 0.0090, 0.0214, 0.0251, 0.0209,\n",
            "        0.0203, 0.0188, 0.0148, 0.0167, 0.0111, 0.0193, 0.0098, 0.0084, 0.0237,\n",
            "        0.0163, 0.0233, 0.0237, 0.0480, 0.0494, 0.0209, 0.0189, 0.0308, 0.0277,\n",
            "        0.0199, 0.0432, 0.0478, 0.0505, 0.0232, 0.0395, 0.0190, 0.0136, 0.0219,\n",
            "        0.0331, 0.0220, 0.0212, 0.0385, 0.0229, 0.0517, 0.0227, 0.0508, 0.0410,\n",
            "        0.0654, 0.0231, 0.0467, 0.0359, 0.0511, 0.0351, 0.0238, 0.0249, 0.0240,\n",
            "        0.0755, 0.0596, 0.0302, 0.0705, 0.0619, 0.0389, 0.0551, 0.0214, 0.0155,\n",
            "        0.0563, 0.0421, 0.0329, 0.0478, 0.0233, 0.0218, 0.0104, 0.0310, 0.0257,\n",
            "        0.0153, 0.0590, 0.0352, 0.0427, 0.0594, 0.0197, 0.0338, 0.0217, 0.0426,\n",
            "        0.0287, 0.0390, 0.0309, 0.0366, 0.0201, 0.0170, 0.0313, 0.0446, 0.0494,\n",
            "        0.0470, 0.0560, 0.0593, 0.0317, 0.0654, 0.0739, 0.0639, 0.0755, 0.0842,\n",
            "        0.0574, 0.0843, 0.0635, 0.0749, 0.0918, 0.0890, 0.0736, 0.0871, 0.0863,\n",
            "        0.0805, 0.0968, 0.0719, 0.0864, 0.0784, 0.0859, 0.0975, 0.0768, 0.1087,\n",
            "        0.0431, 0.0757, 0.0619, 0.0714, 0.0366, 0.0812, 0.0438, 0.0460, 0.0660,\n",
            "        0.0384, 0.0504, 0.0236, 0.0418, 0.0506, 0.0482, 0.0531, 0.0604, 0.0300,\n",
            "        0.0371, 0.0428, 0.0321, 0.0131, 0.0280, 0.0633, 0.0481, 0.0588, 0.0894,\n",
            "        0.0501, 0.0409, 0.0183, 0.0252, 0.0426, 0.0398, 0.0261, 0.0561, 0.0281,\n",
            "        0.0374, 0.0137, 0.0244, 0.0264, 0.0368, 0.0361, 0.0454, 0.0462, 0.0088,\n",
            "        0.0256, 0.0354, 0.0235, 0.0203, 0.0331, 0.0594, 0.0285, 0.0168, 0.0228,\n",
            "        0.0319, 0.0411, 0.0028, 0.0271, 0.0417, 0.0349, 0.0558, 0.0202, 0.0353,\n",
            "        0.0131, 0.0480, 0.0247, 0.0128, 0.0155, 0.0157, 0.0132, 0.0048, 0.0500,\n",
            "        0.0358, 0.0196, 0.0286, 0.0383, 0.0124, 0.0245, 0.0128, 0.0207, 0.0114,\n",
            "        0.0020, 0.0189, 0.0311, 0.0495, 0.0367, 0.0182, 0.0250, 0.0105, 0.0309,\n",
            "        0.0323, 0.0322, 0.0347, 0.0294, 0.0410, 0.0279, 0.0157, 0.0060, 0.0160,\n",
            "        0.0153, 0.0256, 0.0348, 0.0110, 0.0283, 0.0220, 0.0243, 0.0299, 0.0456,\n",
            "        0.0408, 0.0126, 0.0481, 0.0208, 0.0020, 0.0415, 0.0330, 0.0000, 0.0290,\n",
            "        0.0359, 0.0118, 0.0138, 0.0153, 0.0174, 0.0182, 0.0253, 0.0236, 0.0090,\n",
            "        0.0062, 0.0041, 0.0093, 0.0058, 0.0530, 0.0120, 0.0115, 0.0198, 0.0188,\n",
            "        0.0230, 0.0203, 0.0183, 0.0284, 0.0233, 0.0031, 0.0140, 0.0093, 0.0055,\n",
            "        0.0036, 0.0232, 0.0086, 0.0133, 0.0039, 0.0239, 0.0176, 0.0187, 0.0266,\n",
            "        0.0309, 0.0176, 0.0236, 0.0047, 0.0051, 0.0196])\n",
            "[[[[-0.27026138 -0.26334506 -0.25642872 ... -0.44736332 -0.43376002\n",
            "    -0.43376002]\n",
            "   [-0.2628863  -0.26480362 -0.26672095 ... -0.4519883  -0.43137145\n",
            "    -0.43137145]\n",
            "   [-0.2555112  -0.26626217 -0.27701315 ... -0.45661333 -0.4289829\n",
            "    -0.4289829 ]\n",
            "   ...\n",
            "   [ 1.1777947   1.1746781   1.1715615  ...  1.0681232   1.0258498\n",
            "     1.0258498 ]\n",
            "   [ 1.1385256   1.135035    1.1315445  ...  1.0497652   0.9997403\n",
            "     0.9997403 ]\n",
            "   [ 1.1385256   1.135035    1.1315445  ...  1.0497652   0.9997403\n",
            "     0.9997403 ]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iBZ1D36UYrz"
      },
      "source": [
        "### Post process the TensorFlow Lite ouput and save the final image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLfj-qKPkaxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ceee7f3-49ff-462e-9fa4-66618b617ca7"
      },
      "source": [
        "depth = resize_depth(torch.from_numpy(output_data), 480, 640)\n",
        "\n",
        "filename = os.path.join(\n",
        "    \"/content/\", os.path.splitext(os.path.basename(\"chris_tflite\"))[0]\n",
        ")\n",
        "np.save(filename + '.npy', depth)\n",
        "write_depth(filename, depth, bits=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[-0.2703, -0.2633, -0.2564,  ..., -0.4474, -0.4338, -0.4338],\n",
            "          [-0.2629, -0.2648, -0.2667,  ..., -0.4520, -0.4314, -0.4314],\n",
            "          [-0.2555, -0.2663, -0.2770,  ..., -0.4566, -0.4290, -0.4290],\n",
            "          ...,\n",
            "          [ 1.1778,  1.1747,  1.1716,  ...,  1.0681,  1.0258,  1.0258],\n",
            "          [ 1.1385,  1.1350,  1.1315,  ...,  1.0498,  0.9997,  0.9997],\n",
            "          [ 1.1385,  1.1350,  1.1315,  ...,  1.0498,  0.9997,  0.9997]]]])\n",
            "torch.Size([384, 384])\n",
            "[[-0.26465985 -0.26431742 -0.26360548 ... -0.45044664 -0.43826064\n",
            "  -0.43216765]\n",
            " [-0.2641645  -0.26480362 -0.2650305  ... -0.4519883  -0.43824375\n",
            "  -0.43137145]\n",
            " [-0.2633129  -0.26575688 -0.26762018 ... -0.4539023  -0.43785343\n",
            "  -0.42982897]\n",
            " ...\n",
            " [ 1.175717    1.1746781   1.1755806  ...  1.0681232   1.039941\n",
            "   1.0258498 ]\n",
            " [ 1.1493714   1.1482494   1.1484095  ...  1.0558846   1.0242572\n",
            "   1.0084435 ]\n",
            " [ 1.1361985   1.135035    1.1348239  ...  1.0497652   1.0164152\n",
            "   0.9997403 ]]\n",
            "-0.48215917\n",
            "1.657558\n",
            "65535\n",
            "[[ 6661.543   6668.885   6682.934  ...  1157.9032  1400.5015  1531.1343]\n",
            " [ 6667.6113  6666.535   6671.226  ...  1148.5618  1403.573   1540.8883]\n",
            " [ 6676.7144  6663.0117  6653.665  ...  1134.5524  1408.179   1555.5201]\n",
            " ...\n",
            " [49970.32   49946.266  49938.41   ... 46622.68   45993.043  45654.    ]\n",
            " [49728.246  49703.656  49692.156  ... 46494.4    45844.18   45494.062 ]\n",
            " [49566.867  49541.92   49527.996  ... 46408.875  45744.95   45387.438 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}